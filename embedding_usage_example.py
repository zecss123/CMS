#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
EmbeddingåŠŸèƒ½ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•åœ¨å®é™…åº”ç”¨ä¸­ä½¿ç”¨embeddingåŠŸèƒ½
"""

import sys
import os
import numpy as np
from typing import List, Tuple
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from api.embedding_client import EmbeddingClient

def calculate_similarity(vec1: List[float], vec2: List[float]) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦
    
    Args:
        vec1: ç¬¬ä¸€ä¸ªå‘é‡
        vec2: ç¬¬äºŒä¸ªå‘é‡
        
    Returns:
        ä½™å¼¦ç›¸ä¼¼åº¦å€¼ (-1 åˆ° 1)
    """
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    dot_product = np.dot(v1, v2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    
    if norm1 == 0 or norm2 == 0:
        return 0.0
    
    return dot_product / (norm1 * norm2)

def find_most_similar(query_text: str, candidate_texts: List[str], client: EmbeddingClient) -> List[Tuple[str, float]]:
    """
    æ‰¾åˆ°ä¸æŸ¥è¯¢æ–‡æœ¬æœ€ç›¸ä¼¼çš„å€™é€‰æ–‡æœ¬
    
    Args:
        query_text: æŸ¥è¯¢æ–‡æœ¬
        candidate_texts: å€™é€‰æ–‡æœ¬åˆ—è¡¨
        client: Embeddingå®¢æˆ·ç«¯
        
    Returns:
        æŒ‰ç›¸ä¼¼åº¦æ’åºçš„(æ–‡æœ¬, ç›¸ä¼¼åº¦)åˆ—è¡¨
    """
    # ç”ŸæˆæŸ¥è¯¢æ–‡æœ¬çš„embedding
    query_embedding = client.get_single_embedding(query_text, use_test_data=True)
    
    # ç”Ÿæˆæ‰€æœ‰å€™é€‰æ–‡æœ¬çš„embedding
    candidate_embeddings = []
    for text in candidate_texts:
        embedding = client.get_single_embedding(text, use_test_data=True)
        candidate_embeddings.append(embedding)
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = []
    for i, candidate_embedding in enumerate(candidate_embeddings):
        similarity = calculate_similarity(query_embedding, candidate_embedding)
        similarities.append((candidate_texts[i], similarity))
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    similarities.sort(key=lambda x: x[1], reverse=True)
    
    return similarities

def demo_text_similarity():
    """
    æ¼”ç¤ºæ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—
    """
    print("ğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—æ¼”ç¤º")
    print("=" * 50)
    
    client = EmbeddingClient()
    
    # æŸ¥è¯¢æ–‡æœ¬
    query = "æŒ¯åŠ¨ç›‘æµ‹ç³»ç»Ÿæ•…éšœè¯Šæ–­"
    
    # å€™é€‰æ–‡æœ¬åº“
    candidates = [
        "è®¾å¤‡æŒ¯åŠ¨å¼‚å¸¸æ£€æµ‹æ–¹æ³•",
        "æœºæ¢°æ•…éšœè¯Šæ–­æŠ€æœ¯ç ”ç©¶",
        "æ¸©åº¦ä¼ æ„Ÿå™¨æ ¡å‡†ç¨‹åº",
        "æŒ¯åŠ¨ä¿¡å·é¢‘è°±åˆ†æ",
        "è®¾å¤‡ç»´æŠ¤ä¿å…»è®¡åˆ’",
        "æ•…éšœé¢„è­¦ç³»ç»Ÿè®¾è®¡",
        "æ•°æ®é‡‡é›†ç³»ç»Ÿé…ç½®",
        "æŒ¯åŠ¨æµ‹è¯•æ ‡å‡†è§„èŒƒ"
    ]
    
    print(f"ğŸ¯ æŸ¥è¯¢æ–‡æœ¬: {query}")
    print(f"ğŸ“š å€™é€‰æ–‡æœ¬æ•°é‡: {len(candidates)}")
    print("\nğŸ”„ æ­£åœ¨è®¡ç®—ç›¸ä¼¼åº¦...")
    
    # è®¡ç®—ç›¸ä¼¼åº¦
    similarities = find_most_similar(query, candidates, client)
    
    print("\nğŸ“Š ç›¸ä¼¼åº¦æ’åºç»“æœ:")
    print("-" * 40)
    
    for i, (text, similarity) in enumerate(similarities, 1):
        similarity_percent = similarity * 100
        print(f"{i:2d}. {text:<20} | ç›¸ä¼¼åº¦: {similarity_percent:6.2f}%")
    
    # åˆ†æç»“æœ
    print("\nğŸ’¡ ç»“æœåˆ†æ:")
    top_3 = similarities[:3]
    for i, (text, similarity) in enumerate(top_3, 1):
        if similarity > 0.8:
            level = "é«˜åº¦ç›¸å…³"
        elif similarity > 0.6:
            level = "ä¸­åº¦ç›¸å…³"
        elif similarity > 0.4:
            level = "ä½åº¦ç›¸å…³"
        else:
            level = "ä¸ç›¸å…³"
        print(f"  {i}. {text} - {level}")

def demo_text_clustering():
    """
    æ¼”ç¤ºæ–‡æœ¬èšç±»
    """
    print("\n\nğŸ¯ æ–‡æœ¬èšç±»æ¼”ç¤º")
    print("=" * 50)
    
    client = EmbeddingClient()
    
    # æµ‹è¯•æ–‡æœ¬é›†åˆ
    texts = [
        "æŒ¯åŠ¨ç›‘æµ‹è®¾å¤‡å®‰è£…",
        "æ¸©åº¦ä¼ æ„Ÿå™¨é…ç½®",
        "è®¾å¤‡æ•…éšœè¯Šæ–­",
        "æŒ¯åŠ¨ä¿¡å·åˆ†æ",
        "æ¸©åº¦æ•°æ®é‡‡é›†",
        "æ•…éšœé¢„è­¦ç³»ç»Ÿ",
        "æŒ¯åŠ¨é¢‘è°±æ£€æµ‹",
        "æ¸©åº¦å¼‚å¸¸æŠ¥è­¦"
    ]
    
    print(f"ğŸ“ æ–‡æœ¬æ•°é‡: {len(texts)}")
    print("\nğŸ”„ æ­£åœ¨ç”Ÿæˆembeddingå‘é‡...")
    
    # ç”Ÿæˆæ‰€æœ‰æ–‡æœ¬çš„embedding
    embeddings = []
    for text in texts:
        embedding = client.get_single_embedding(text, use_test_data=True)
        embeddings.append(embedding)
    
    print("\nğŸ“Š æ–‡æœ¬ç›¸ä¼¼åº¦çŸ©é˜µ:")
    print("    ", end="")
    for i in range(len(texts)):
        print(f"{i:4d}", end="")
    print()
    
    # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
    similarity_matrix = []
    for i in range(len(embeddings)):
        row = []
        print(f"{i:2d}: ", end="")
        for j in range(len(embeddings)):
            similarity = calculate_similarity(embeddings[i], embeddings[j])
            row.append(similarity)
            print(f"{similarity:4.2f}", end="")
        similarity_matrix.append(row)
        print(f"  {texts[i][:15]}...")
    
    # ç®€å•èšç±»åˆ†æ
    print("\nğŸ” èšç±»åˆ†æ:")
    threshold = 0.7
    clusters = []
    used = set()
    
    for i in range(len(texts)):
        if i in used:
            continue
        
        cluster = [i]
        used.add(i)
        
        for j in range(i + 1, len(texts)):
            if j not in used and similarity_matrix[i][j] > threshold:
                cluster.append(j)
                used.add(j)
        
        clusters.append(cluster)
    
    for i, cluster in enumerate(clusters, 1):
        print(f"\nğŸ“‚ èšç±» {i}:")
        for idx in cluster:
            print(f"  - {texts[idx]}")

def demo_semantic_search():
    """
    æ¼”ç¤ºè¯­ä¹‰æœç´¢
    """
    print("\n\nğŸ” è¯­ä¹‰æœç´¢æ¼”ç¤º")
    print("=" * 50)
    
    client = EmbeddingClient()
    
    # çŸ¥è¯†åº“
    knowledge_base = [
        "æŒ¯åŠ¨ç›‘æµ‹ç³»ç»Ÿç”¨äºå®æ—¶ç›‘æ§è®¾å¤‡è¿è¡ŒçŠ¶æ€",
        "æ¸©åº¦ä¼ æ„Ÿå™¨å¯ä»¥æ£€æµ‹è®¾å¤‡çš„çƒ­çŠ¶æ€å˜åŒ–",
        "æ•…éšœè¯Šæ–­éœ€è¦ç»“åˆå¤šç§ä¼ æ„Ÿå™¨æ•°æ®è¿›è¡Œåˆ†æ",
        "é¢‘è°±åˆ†ææ˜¯æŒ¯åŠ¨ä¿¡å·å¤„ç†çš„é‡è¦æ–¹æ³•",
        "é¢„è­¦ç³»ç»Ÿå¯ä»¥æå‰å‘ç°è®¾å¤‡å¼‚å¸¸æƒ…å†µ",
        "æ•°æ®é‡‡é›†ç³»ç»Ÿè´Ÿè´£æ”¶é›†å„ç§ä¼ æ„Ÿå™¨ä¿¡æ¯",
        "è®¾å¤‡ç»´æŠ¤åº”è¯¥æ ¹æ®ç›‘æµ‹æ•°æ®åˆ¶å®šè®¡åˆ’",
        "ä¿¡å·å¤„ç†ç®—æ³•ç”¨äºæå–æœ‰ç”¨çš„ç‰¹å¾ä¿¡æ¯"
    ]
    
    # æœç´¢æŸ¥è¯¢
    queries = [
        "å¦‚ä½•ç›‘æ§è®¾å¤‡çŠ¶æ€ï¼Ÿ",
        "æ¸©åº¦å¼‚å¸¸æ€ä¹ˆæ£€æµ‹ï¼Ÿ",
        "æŒ¯åŠ¨ä¿¡å·å¦‚ä½•åˆ†æï¼Ÿ"
    ]
    
    print(f"ğŸ“š çŸ¥è¯†åº“æ¡ç›®: {len(knowledge_base)}")
    print(f"ğŸ” æœç´¢æŸ¥è¯¢: {len(queries)}")
    
    for query in queries:
        print(f"\nâ“ æŸ¥è¯¢: {query}")
        print("ğŸ“‹ æœç´¢ç»“æœ:")
        
        similarities = find_most_similar(query, knowledge_base, client)
        
        # æ˜¾ç¤ºå‰3ä¸ªæœ€ç›¸å…³çš„ç»“æœ
        for i, (text, similarity) in enumerate(similarities[:3], 1):
            similarity_percent = similarity * 100
            print(f"  {i}. [{similarity_percent:5.1f}%] {text}")

def main():
    """
    ä¸»å‡½æ•°
    """
    print("ğŸš€ EmbeddingåŠŸèƒ½åº”ç”¨ç¤ºä¾‹")
    print(f"ğŸ“ å·¥ä½œç›®å½•: {os.getcwd()}")
    print(f"â° è¿è¡Œæ—¶é—´: {os.popen('date').read().strip()}")
    print("=" * 60)
    
    try:
        # è¿è¡Œå„ç§æ¼”ç¤º
        demo_text_similarity()
        demo_text_clustering()
        demo_semantic_search()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰æ¼”ç¤ºå®Œæˆï¼")
        print("\nğŸ’¡ åº”ç”¨åœºæ™¯æ€»ç»“:")
        print("- ğŸ” æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®— - æ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
        print("- ğŸ¯ æ–‡æœ¬èšç±»åˆ†æ - è‡ªåŠ¨åˆ†ç»„ç›¸ä¼¼å†…å®¹")
        print("- ğŸ” è¯­ä¹‰æœç´¢ - åŸºäºå«ä¹‰è€Œéå…³é”®è¯æœç´¢")
        print("- ğŸ“Š å†…å®¹æ¨è - æ¨èç›¸å…³æ–‡æ¡£æˆ–ä¿¡æ¯")
        print("- ğŸ¤– æ™ºèƒ½é—®ç­” - åŒ¹é…æœ€ç›¸å…³çš„ç­”æ¡ˆ")
        print("\nâœ¨ è¿™äº›åŠŸèƒ½éƒ½åŸºäºembeddingå‘é‡çš„æ•°å­¦è¿ç®—å®ç°")
        
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()